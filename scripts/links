#!/usr/bin/env python3

import argparse
import csv
import sqlite3
import pyutils
import sys
from datetime import datetime, timezone
from html.parser import HTMLParser
from urllib.request import urlopen, Request

DB_PATH = '/home/fed/db/links.db'
SCHEMA_PATH = '/home/fed/db/links_schema.sql'
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'
BOLD = '\u001b[1m'
RESET = '\u001b[0m'

def create_or_open_db():
    with open(SCHEMA_PATH, 'r') as schema_file:
        schema = schema_file.read()
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.executescript(schema)
    con.commit()
    return con, cur

def link_exists(link_id):
    con, cur = create_or_open_db()
    res = cur.execute('SELECT COUNT(*) FROM links WHERE id = ?', (link_id,))
    count, = res.fetchone()
    con.close()
    return count == 1

def fetch_tags_rows():
    con, cur = create_or_open_db()
    res = cur.execute('SELECT tags FROM links;')
    rows = res.fetchall()
    con.close()
    return [row[0] for row in rows]

def fetch_all_tags():
    tags = set()
    for comma_separated_tags in fetch_tags_rows():
        for tag in comma_separated_tags.split(','):
            tags.add(tag)
    return list(sorted(tags))

def now_utc():
    return datetime.strftime(datetime.now(timezone.utc), '%Y-%m-%d %H:%M:%S')

def print_result_table(rows):
    for idx, row in enumerate(rows):
        id, url, title, desc, _, _= row
        print(f'{id}: {BOLD}{desc}{RESET}')
        print(f'└─── {url}')

def get_title_from_url(url):
    request = Request(url, headers={'User-Agent': USER_AGENT})
    try:
        response = urlopen(request)
        encoding = response.info().get_content_charset()
        html = response.read().decode(encoding if encoding else 'utf-8')
        class TitleParser(HTMLParser):
            def __init__(self):
                HTMLParser.__init__(self)
                self.match = False
                self.title = ''
            def handle_starttag(self, tag, attributes):
                self.match = tag == 'title'
            def handle_data(self, data):
                if self.match:
                    self.title = data
                    self.match = False
        title_parser = TitleParser()
        title_parser.feed(html)
        return title_parser.title.strip()
    except Exception:
        return ''

def stringify_row(row):
    id, url, title, description, tags, date_added = row
    return f'{id} {url} {title} {description} {tags} {date_added}'

def list_links(args):
    con, cur = create_or_open_db()
    res = cur.execute('SELECT * FROM links;')
    rows = res.fetchall()
    con.close()
    if not rows:
        return
    if 'csv' in args and args.csv:
        for row in rows:
            (id, url, title, description, tags, date_added) = row
            print(f'"{id}","{url}","{title}","{description}","{tags}","{date_added}"')
    else:
        print_result_table(rows)

def add_link(args):
    title = get_title_from_url(args.url)
    description = args.description if args.description else ''
    if args.tags:
        tags = args.tags
    else:
        tags = pyutils.fzf(fetch_all_tags(), '--height=12 --multi').replace('\n', ',')
    con, cur = create_or_open_db()
    query = '''
        INSERT INTO links (rowid, url, title, description, tags, date_added)
        VALUES (NULL, ?, ?, ?, ?, ?);
    '''
    cur.execute(query, (args.url, title, description, tags, now_utc()))
    con.commit()
    con.close()

# TODO: Allow for selecting one or more links to remove via fzf-based fuzzy
#       finding interface, searching on url/title/description/tags
def remove_link(args):
    if not link_exists(args.id):
        print(f'Link id {args.id} not found')
        sys.exit(1)
    con, cur = create_or_open_db()
    cur.execute('DELETE FROM links WHERE id = ?', (args.id,))
    con.commit()
    con.close()

def update_link(args):
    if not link_exists(args.id):
        print(f'Link id {args.id} not found')
        sys.exit(1)
    con, cur = create_or_open_db()
    if not args.url and not args.description and not args.tags:
        print('Must update at least one of url, description, tags')
        sys.exit(1)
    if args.url:
        cur.execute('UPDATE links SET url = ? WHERE id = ?', (args.url, args.id))
    if args.description:
        cur.execute('UPDATE links SET description = ? WHERE id = ?', (args.description, args.id))
    if args.tags:
        new_tags = args.tags
        if '%t' in args.tags:
            # %t represents the existing tags and is used to make easier modifications.
            old_tags, = cur.execute('SELECT tags FROM links WHERE id = ?', (args.id,)).fetchone()
            new_tags = args.tags.replace('%t', old_tags)
        cur.execute('UPDATE links SET tags = ? WHERE id = ?', (new_tags, args.id))
    con.commit()
    con.close()

def find_link(args):
    con, cur = create_or_open_db()
    if args.query:
        res = cur.execute('SELECT rowid, * FROM links_fts(?) ORDER BY rank;', (args.query,))
        rows = res.fetchall()
    else:
        res = cur.execute('SELECT * FROM links;')
        rows = res.fetchall()
        stringified_rows_to_rows = {stringify_row(row): row for row in rows}
        fzf_input = stringified_rows_to_rows.keys()
        fzf_selection = pyutils.fzf(fzf_input)
        if fzf_selection:
            rows = [stringified_rows_to_rows[fzf_selection]]
        else:
            rows = []
    con.close()
    if not rows:
        return
    if args.sort == 'date':
        rows.sort(key=lambda row: row[5]) # date_added
    if args.csv:
        for row in rows:
            id, url, title, description, tags, date_added = row
            print(f'"{id}","{url}","{title}","{description}","{tags}","{date_added}"')
    else:
        print_result_table(rows)

def open_link(args):
    if 'id' in args and args.id:
        if not link_exists(args.id):
            print(f'Link id {args.id} not found')
            sys.exit(1)
        con, cur = create_or_open_db()
        res = cur.execute('SELECT url FROM links WHERE id = ?', (args.id,))
        url, = res.fetchone()
        con.close()
    else:
        con, cur = create_or_open_db()
        res = cur.execute('SELECT * FROM links;')
        rows = res.fetchall()
        con.close()
        stringified_rows_to_rows = {stringify_row(row): row for row in rows}
        fzf_input = stringified_rows_to_rows.keys()
        fzf_selection = pyutils.fzf(fzf_input)
        if fzf_selection:
            _, url, _, _, _, _ = stringified_rows_to_rows[fzf_selection]
        else:
            return
    pyutils.sh(f'xdg-open {url}')

def load_links(args):
    with open(args.csv_file, newline='') as csv_file:
        reader = csv.reader(csv_file, delimiter=',', quotechar='"')
        data = []
        for row in reader:
            if not row:
                continue
            url, description, tags = row
            print(f'Loading {url}...')
            title = get_title_from_url(url)
            data.append({
                "url": url,
                "title": title,
                "description": description,
                "tags": tags,
                "date_added": now_utc(),
            })
        con, cur = create_or_open_db()
        cur.executemany(
            'INSERT INTO links VALUES(NULL, :url, :title, :description, :tags, :date_added);',
            data
        )
        con.commit()
        con.close()

def list_urls(args):
    con, cur = create_or_open_db()
    res = cur.execute('SELECT url FROM links ORDER BY date_added ASC;')
    rows = res.fetchall()
    con.close()
    for row in rows:
        url, = row
        print(f'{url}')

def list_tags(args):
    if args.query:
        # TODO: Consider whether we should instead allow the match query to be
        #       used directly as-passed (e.g. links tags '"tag1" OR "tag2"')
        match = ' '.join([f'"{tag.strip()}"' for tag in args.query.split(',')])
        con, cur = create_or_open_db()
        res = cur.execute("SELECT rowid, url, tags FROM links_fts WHERE tags MATCH ?", (match,))
        rows = res.fetchall()
        con.close()
        # TODO: print nice table like for find
        for row in rows:
            print(row)
        return
    if args.histogram:
        tags = {}
        for comma_separated_tags in fetch_tags_rows():
            for tag in comma_separated_tags.split(','):
                if tag not in tags:
                    tags[tag] = 1
                else:
                    tags[tag] += 1
        tag_histogram = {tag: count for tag, count in sorted(tags.items(), key=lambda item: item[1], reverse=True)}
        for tag in tag_histogram:
            print(f'{tag}, {tag_histogram[tag]}')
    else:
        # Simply print all tags alphabetically.
        for tag in fetch_all_tags():
            print(tag)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers()
    parser.set_defaults(func=list_links)

    parser_list = subparsers.add_parser('ls', help='list links')
    parser_list.add_argument('--csv', action='store_true', help='display in csv format')
    parser_list.set_defaults(func=list_links)

    parser_add = subparsers.add_parser('add', help='add new link')
    parser_add.add_argument('url', help='link url')
    parser_add.add_argument('-d', '--description', help='description for link')
    parser_add.add_argument('-t', '--tags', help='comma-separated list of tags')
    parser_add.set_defaults(func=add_link)

    parser_rm = subparsers.add_parser('rm', help='remove link')
    parser_rm.add_argument('id', type=int, help='link id (taken from \'links ls\')')
    parser_rm.set_defaults(func=remove_link)

    parser_update = subparsers.add_parser('update', help='update link')
    parser_update.add_argument('id', type=int, help='link id (taken from \'links ls\')')
    parser_update.add_argument('-u', '--url', help='new url')
    parser_update.add_argument('-d', '--description', help='new description')
    parser_update.add_argument('-t', '--tags', help='new tags')
    parser_update.set_defaults(func=update_link)

    parser_find = subparsers.add_parser('find', help='find links')
    parser_find.add_argument('query', nargs='?', help='search query')
    parser_find.add_argument('--csv', action='store_true', help='display in csv format')
    parser_find.add_argument('--sort', choices=['relevance', 'date'], default='relevance', help='sort results')
    parser_find.set_defaults(func=find_link)

    parser_open = subparsers.add_parser('open', help='open links in browser')
    parser_open.add_argument('id', type=int, nargs='?', help='link id (taken from \'links ls\')')
    parser_open.set_defaults(func=open_link)

    parser_load = subparsers.add_parser('load', help='load links from csv (url,desc,tags)')
    parser_load.add_argument('csv_file', help='csv file (url,desc,tags)')
    parser_load.set_defaults(func=load_links)

    parser_urls = subparsers.add_parser('urls', help='list all urls in insertion order')
    parser_urls.set_defaults(func=list_urls)

    parser_tags = subparsers.add_parser('tags', help='list all tags alphabetically')
    parser_tags_group = parser_tags.add_mutually_exclusive_group()
    parser_tags_group.add_argument('query', nargs='?', help='find entries with tags matching query')
    parser_tags_group.add_argument('--histogram', action='store_true', help='show tags and counts')
    parser_tags.set_defaults(func=list_tags)

    args = parser.parse_args()
    args.func(args)
